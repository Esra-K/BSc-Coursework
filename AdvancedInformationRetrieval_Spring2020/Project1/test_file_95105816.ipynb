{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_file_95105816.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZlYiGMMSmaq",
        "colab_type": "text"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"B Zar\" size=18>\n",
        "\n",
        "تغییرات فایل تست:\n",
        "<ul>\n",
        "<li>\n",
        "برای طول posting_list از تابع length_of_posting_list به جای len استفاده کردم که آیتم‌های خالی رو حساب نمی‌کنه\n",
        "</li>\n",
        "<li>\n",
        "در آدرس‌ها data/ را از اول data/Persian.xml برداشتم\n",
        "</li>\n",
        "<li>\n",
        "به آخر storage/backup یک .txt اضافه کردم\n",
        "</li>\n",
        "<li>\n",
        "در functions[f](doc) به جای doc، str(doc) گذاشتم\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "تغییرات کد:\n",
        "<ul>\n",
        "<li>\n",
        "در تابع detailed_search و هر کدام از ۴ تابع evaluation، یک خط پرینت نتیجه بود که کامنت کردم\n",
        "</li>\n",
        "<li>\n",
        "تابع‌های search و detailed_search بیست تا نتیجه‌ی اول را برمی‌گرداندند که ۲۰ را به ۱۵ تغییر دادم\n",
        "</li>\n",
        "<li>\n",
        "در add_document_to_indexes و delete_document_from_indexes آیدی داک به آخر مسیر اضافه شده (خط ۸۹ و ۱۱۲) آن را پاک کردم و به جای آن بعد از خط ۹۶ و ۱۱۹ یک if pid == doc_num گذاشتم تا داک را پیدا کند. تا آخر تابع جزو if می‌شود\n",
        "</li>\n",
        "<li>\n",
        "در خط ۳۷۶ و ۴۲۴، مخرج را با $2^{-16}$ جمع کردم. (تابع آماده‌ی precision_recall_fscore_support هم وقتی precision و recall با هم 0 می‌شوند به صورت پیش‌فرض هشدار می‌دهد و عددی را بر‌نمی‌گرداند)\n",
        "</li>\n",
        "</ul>\n",
        "</font>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UD7vwJOSMcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def length_of_posting_list(p):\n",
        "    return len([a1 for a1, v in p.items() if len(v[\"title\"]) > 0 or len(v[\"text\"]) > 0])\n",
        "\n",
        "\n",
        "word1 = 'فکری'\n",
        "doc_id = 3014\n",
        "\n",
        "word2 = 'هیلاندراس'\n",
        "doc_id2 = 6752\n",
        "bigram = 'لا'\n",
        "\n",
        "\n",
        "def get_count(l):\n",
        "    i = [1 for _, t in l.items() for q in t['text']]\n",
        "    j = [1 for _, t in l.items() if 'title' in t.keys() for q in t['title']]\n",
        "    return len(i) + len(j)\n",
        "\n",
        "\n",
        "def test_prepare_text():\n",
        "    print(\"\\n============ testing 'prepare_text' =============================================\")\n",
        "    raw_text = \"کتابهای مناسبی نوشته شوند ! در راستای ارتقای . سطح آموزش کشور ؟ تلاش‌های زیادی صورت می‌گیرد\"\n",
        "    prepared_text = prepare_text(raw_text)\n",
        "\n",
        "    print(\"prepared text is :\", prepared_text, \"with length:\", len(prepared_text))\n",
        "\n",
        "\n",
        "test_prepare_text()\n",
        "\n",
        "\n",
        "def test_get_posting_list():\n",
        "    print(\"\\n============ testing 'get_posting_list' =========================================\")\n",
        "\n",
        "    prepared_text = prepare_text(word1)[0]\n",
        "    posting_list = get_posting_list(prepared_text)\n",
        "    # posting_list = {3014:{'title':[...] , 'text':[...]}}\n",
        "\n",
        "    #     print (\"posting list for input\" , prepared_text, \"is :\", posting_list , \"with length:\" , len (posting_list))\n",
        "    print(\"number of ocurrences of the word\", word1, \" in documents = \", get_count(posting_list))\n",
        "    print('docs with the word:', sorted(list(posting_list.keys())))\n",
        "\n",
        "\n",
        "test_get_posting_list()\n",
        "\n",
        "\n",
        "def test_doc_remove():\n",
        "    print(\"\\n============ testing 'doc_remove' ================================================\")\n",
        "\n",
        "    posting_list = get_posting_list(prepare_text(word1)[0])\n",
        "    print(\"length of posting list for word\", word1, \"before removing doc\", doc_id, \":\", length_of_posting_list(posting_list))\n",
        "\n",
        "    delete_document_from_indexes('Persian.xml', doc_id)\n",
        "\n",
        "    posting_list = get_posting_list(prepare_text(word1)[0])\n",
        "    print(\"length of posting list for word\", word1, \"after removing doc\", doc_id, \":\", length_of_posting_list(posting_list))\n",
        "\n",
        "    posting_list = get_posting_list(prepare_text(word2)[0])\n",
        "    print(\"length of posting list for word\", word2, \"before removing doc\", doc_id2, \":\", length_of_posting_list(posting_list))\n",
        "\n",
        "    delete_document_from_indexes('Persian.xml', doc_id2)\n",
        "\n",
        "    posting_list = get_posting_list(prepare_text(word2)[0])\n",
        "    print(\"length of posting list for word\", word2, \"after removing doc\", doc_id2, \":\", length_of_posting_list(posting_list))\n",
        "\n",
        "\n",
        "test_doc_remove()\n",
        "\n",
        "\n",
        "def test_doc_add():\n",
        "    print(\"\\n============ testing 'doc_add' ================================================\")\n",
        "\n",
        "    posting_list = get_posting_list(prepare_text(word1)[0])\n",
        "    print(\"length of posting list for word\", word1, \"before adding doc\", doc_id, \":\", length_of_posting_list(posting_list))\n",
        "    print(\"number of ocurrences for \", word1, \":\", get_count(posting_list))\n",
        "\n",
        "    add_document_to_indexes('Persian.xml', doc_id)\n",
        "\n",
        "    posting_list = get_posting_list(prepare_text(word1)[0])\n",
        "    print(\"length of posting list for word\", word1, \"after adding doc\", doc_id, \":\", length_of_posting_list(posting_list))\n",
        "    print(\"number of ocurrences for \", word1, \":\", get_count(posting_list))\n",
        "\n",
        "    posting_list = get_posting_list(prepare_text(word2)[0])\n",
        "    print(\"length of posting list for word\", word2, \"before adding doc\", doc_id2, \":\", length_of_posting_list(posting_list))\n",
        "\n",
        "    add_document_to_indexes('Persian.xml', doc_id2)\n",
        "\n",
        "    posting_list = get_posting_list(prepare_text(word2)[0])\n",
        "    print(\"length of posting list for word\", word2, \"after adding doc\", doc_id2, \":\", length_of_posting_list(posting_list))\n",
        "\n",
        "\n",
        "test_doc_add()\n",
        "\n",
        "\n",
        "def test_save_and_load():\n",
        "    print(\"\\n============ testing save and load methods ========================================\")\n",
        "\n",
        "    destination = \"storage/backup.txt\"\n",
        "\n",
        "    posting_list = get_posting_list(prepare_text(word1)[0])\n",
        "    print(\"length of posting list for word\", word1, \"before saving:\", length_of_posting_list(posting_list))\n",
        "    print(\"number of ocurrences for \", word1, \":\", get_count(posting_list))\n",
        "\n",
        "    save_index(destination)\n",
        "    load_index(destination)\n",
        "\n",
        "    posting_list = get_posting_list(prepare_text(word1)[0])\n",
        "    print(\"length of posting list for word\", word1, \"after loading:\", length_of_posting_list(posting_list))\n",
        "    print(\"number of ocurrences for \", word1, \":\", get_count(posting_list))\n",
        "\n",
        "\n",
        "test_save_and_load()\n",
        "\n",
        "\n",
        "def test_search():\n",
        "\n",
        "    ##################################\n",
        "    ## Do not change this part\n",
        "    ##################################\n",
        "    query = 'سیاره های بزرگ \"منظومه شمسی\"'\n",
        "    method = \"ltc-lnc\"\n",
        "    ##################################\n",
        "\n",
        "    relevant_docs = search(query, method)\n",
        "    print(relevant_docs)\n",
        "\n",
        "test_search()\n",
        "\n",
        "\n",
        "def test_detailed_search():\n",
        "    ##################################\n",
        "    ## Do not change this part\n",
        "    ##################################\n",
        "    title_query = 'فهرست شهرهای ایران'\n",
        "    text_query = 'استان گیلان شهرستان لنگرود'\n",
        "    ##################################\n",
        "\n",
        "    relevant_docs = detailed_search(title_query, text_query)\n",
        "    print(relevant_docs)\n",
        "\n",
        "\n",
        "test_detailed_search()\n",
        "\n",
        "try:\n",
        "    python_open\n",
        "    print(\"Already done!\")\n",
        "except NameError:\n",
        "    python_open = open\n",
        "    def open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None):\n",
        "        encoding=\"utf-8\"\n",
        "        return python_open(file, mode=mode, buffering=buffering, encoding=encoding, errors=errors, newline=newline, closefd=closefd, opener=opener)\n",
        "\n",
        "\n",
        "##################################\n",
        "## Do not change this part\n",
        "##################################\n",
        "test_docs = ['all', 1, 2, 3]\n",
        "functions = {'R_Precision': R_Precision, 'F_measure': F_measure, 'MAP': MAP, 'NDCG': NDCG}\n",
        "##################################\n",
        "\n",
        "for doc in test_docs:\n",
        "    print(\"{}\\ndoc:\\t{}\".format('-' * 30, doc))\n",
        "    for f in functions.keys():\n",
        "        out = functions[f](doc)\n",
        "        print(\"{:11}:\\t{:.2f}\".format(f, out))\n",
        "\n",
        "##################################\n",
        "## Do not change this part\n",
        "##################################\n",
        "test_docs = [1, 2, 3]\n",
        "rels = [\n",
        "    [6753, 7134, 6978, 7136, 4530, 6798, 6885, 5381, 6900, 4537, 5509, 6794, 4094, 6417, 3666, 5967],\n",
        "    [6753, 5509, 4718, 6798, 6850, 6417, 6978, 6871],\n",
        "    list(range(20))\n",
        "]\n",
        "outputs = [{'R_Precision': 1.0, 'F_measure': 0.967741935483871, 'MAP': 0.9375, 'NDCG': 0.9635640110263509},\n",
        "           {'R_Precision': 0.4444444444444444, 'F_measure': 0.6153846153846153, 'MAP': 0.4444444444444444, 'NDCG': 0.6313802022799658},\n",
        "           {'R_Precision': 0.0, 'F_measure': 0.0, 'MAP': 0.0, 'NDCG': 0.0}]\n",
        "\n",
        "functions = {'R_Precision':R_Precision, 'F_measure':F_measure, 'MAP':MAP, 'NDCG': NDCG}\n",
        "##################################\n",
        "idx = 0\n",
        "\n",
        "ds = detailed_search\n",
        "s = search\n",
        "def detailed_search(title_query, text_query, method=\"ltn-lnn\"):\n",
        "    return rels[idx]\n",
        "\n",
        "def search(query, method=\"ltn-lnn\", weight=2):\n",
        "    return rels[idx]\n",
        "\n",
        "for f in functions.keys():\n",
        "    print(\"{}\\n{}:\".format('-'*30, f))\n",
        "    idx = 0\n",
        "    for doc in test_docs:\n",
        "        out = functions[f](str(doc))\n",
        "        expected = outputs[idx][f]\n",
        "        print(\"{}:\\t{:.2f}\\t{}\".format(doc, out, abs(out-expected)<1e-3))\n",
        "        idx += 1\n",
        "\n",
        "detailed_search = ds\n",
        "search = s"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}